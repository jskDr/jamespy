from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import Draw
from rdkit import DataStructs

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import linear_model

# This is James Sungjin Kim's library
import jutil
import jchem

from maml.gp import gaussian_process as gp

def list_indices( l, target):
	return [i for i,val in enumerate(l) if val == target]

def get_duplist( x_list, disp = True):
	"""
	Duplication indices are returned.
	"""
	duplist = []
	for x in set( x_list):
		if x_list.count( x) > 1:
			duplist.append( list_indices( x_list, x))

	if disp:
		print duplist
		for d in duplist:
			print [x_list[x] for x in d]

	return duplist

def check_mol2smiles( x_smiles_1):	
	"""
	Find smiles codes which can not operate in rdkit now. 
	x_smiles_1 is refined by cannonical smiles generated by rdkit
	"""
	x_mol_list = [Chem.MolFromSmiles(x) for x in x_smiles_1]

	fail_list = []
	for ii in range( len(x_mol_list)):
		try: 
			x_smiles_1[ii] = Chem.MolToSmiles( x_mol_list[ii])
			#print ii, "Sucess" 
		except:
			print ii, "Faliue"
			fail_list.append( ii)
			x_smiles_1[ii] = ''

	return fail_list


def get_mol2smiles( x_smiles_1):	
	"""
	Find smiles codes which can not operate in rdkit now. 
	x_smiles_1 is refined by cannonical smiles generated by rdkit
	"""
	x_mol_list = [Chem.MolFromSmiles(x) for x in x_smiles_1]

	fail_list = []
	for ii in range( len(x_mol_list)):
		try: 
			Chem.MolToSmiles( x_mol_list[ii])
			#print ii, "Sucess" 
		except:
			print ii, "Faliue"
			fail_list.append( ii)
			x_smiles_1[ii] = ''

	return fail_list	

def pd_remove_no_mol2smiles( pdr, smiles_id = 'SMILES'):
	"""
	Find not working smiles codes
	"""
	s = pdr[ smiles_id].tolist()
	fail_list = get_mol2smiles( s)

	pdr = pd_remove_faillist_ID( pdr, fail_list)

	return pdr


def pd_refine_smiles( pdr, smiles_id = 'SMILES'):
	"""
	smiles codes are refined by rdkit. 
	"""
	s_l = pdr[ smiles_id]
	m_l = map( Chem.MolFromSmiles, s_l)
	new_s_l = map( Chem.MolToSmiles, m_l)

	pdr[ smiles_id] = new_s_l

	return pdr

def pd_clean_smiles( pdr, smiles_id = 'SMILES'):

	if 'ID' not in pdr.keys():
		raise TypeError( 'pdr should have a key of ID.')

	print '1. All columns each with a smile code not supported in rdkit are removing.'
	pdr1 = pd_remove_no_mol2smiles( pdr, smiles_id = smiles_id)

	print '2. Smiles are refined by rdkit'
	pdr2 = pd_refine_smiles( pdr1, smiles_id = smiles_id)

	print '3. Removing columns with duplicated smiles codes.'
	print '   - you may check properties for the same smiles code molecules:'
	print '     pd_get_dup_smiles_and_property()'
	pdr3 = pd_remove_dup_smiles( pdr2, smiles_id = smiles_id)

	return pdr3


def pd_remove_duplist_ID( pdr, dup_l):

	pdr_ID_x = []
	for d in dup_l:
		pdr_ID_x.append([ pdr.ID.tolist()[x] for x in d])
	print 'pdr_ID_x ->', pdr_ID_x

	pdw = pdr
	for d in pdr_ID_x:
		for x in d[1:]:
			pdw = pdw[ pdw.ID != x]

	#print pdr.SMILES.shape, pdw.SMILES.shape
	return pdw

def pd_remove_faillist_ID( pdr, fail_l):
	"""
	copy ID first and then operate for deleting
	since pdw is chaning on the fly.
	Index of list and index of pd item can not be the same. 
	"""
	#pdr_ID_x = [ pdr.ID[x] for x in fail_l]
	pdr_ID_x = [ pdr.ID.tolist()[x] for x in fail_l]
	print "pdr_ID_x -> ", pdr_ID_x

	pdw = pdr
	for x in pdr_ID_x:
		# If indexing is working, this becomes copy
		# since the length is not any longer the same
		pdw = pdw[ pdw.ID != x]

	#print [pdr.ID[ x] for x in fail_l]
	#print pdr.SMILES.shape, pdw.SMILES.shape
	return pdw	

def pd_check_mol2smiles( pd_smiles):

	smiles_l = pd_smiles.tolist()
	fail_l = check_mol2smiles( smiles_l)
	
	# since siles_l is changed, pd values are also changed.
	pd_smiles = smiles_l

	return fail_l

def pd_check_mol2smi( pdr, smiles_id = 'SMILES'):

	smiles_l = pdr[smiles_id].tolist()
	fail_l = check_mol2smiles( smiles_l)
	
	return fail_l

def pd_remove_dup_smiles( pdr, smiles_id = 'SMILES'):

	s_l = pdr[ smiles_id].tolist()
	d_l = get_duplist( s_l)

	print d_l

	new_pdr = pd_remove_duplist_ID( pdr, d_l)

	return new_pdr

def pd_get_fp_strings( pdr, radius = 4, nBits = 1024, smiles_id = 'SMILES'):
	"""
	Extract smiles codes and then convert them to fingerprint string list
	"""

	s_l = pdr[ smiles_id].tolist()
	m_l = map( Chem.MolFromSmiles, s_l)
	fp_s_l = [AllChem.GetMorganFingerprintAsBitVect(m, radius = radius, nBits = nBits).ToBitString() for m in m_l]

	return fp_s_l

def xM( s_l, radius = 4, nBits = 1024):
	
	m_l = map( Chem.MolFromSmiles, s_l)
	fp_l = [AllChem.GetMorganFingerprintAsBitVect(m, radius = radius, nBits = nBits) for m in m_l]
	
	return np.mat( fp_l)

def pd_get_fpM( pdr, radius = 4, nBits = 1024, smiles_id = 'SMILES'):
	"""
	Extract smiles codes and then convert them to fingerprint matrix.
	"""

	s_l = pdr[ smiles_id].tolist()
	m_l = map( Chem.MolFromSmiles, s_l)
	fp_l = [AllChem.GetMorganFingerprintAsBitVect(m, radius = radius, nBits = nBits) for m in m_l]
	xM  = np.mat( fp_l)

	return xM

pd_get_xM = pd_get_fpM

def pd_get_fpM_fromStr( pdr, fp_id = 'Fingerprint'):
	"""
	Extract fingerprint strings and then convert them to fingerprint matrix.
	"""

	s_l = pdr[ fp_id].tolist()
	fp_i_l2 = [map(int, x) for x in s_l]
	xM  = np.mat( fp_i_l2)

	return xM

def pd_get_yV( pdr, y_id):
	return np.mat( pdr[ y_id]).T

def pd_get_dup_smiles_and_property( pdr, smiles_id = 'SMILES', property_id = 'Solubility_log_mol_l'):

	pdr1 = pd_refine_smiles( pdr, smiles_id = smiles_id)

	lst = get_duplist( pdr1[ smiles_id].tolist(), disp = False)

	print 'SMILES --> Property-1(SMILES), Property-2(SMILES), ...'
	for ll in lst:
		print pdr1[smiles_id][ ll[0]], '-->', 
		for l0 in ll:
			print l0, ":", pdr1[ property_id][l0],
			delta = abs(pdr1[ property_id][ ll[0]] - pdr1[ property_id][l0])
			if delta > 0.1 * abs(pdr1[property_id][ ll[0]]):
				print "Large difference ({})".format( delta),
			elif delta > 0.01 * abs(pdr1[property_id][ ll[0]]):
				print "Medium difference ({})".format( delta),
		print

	print '\n========================================='
	print 'Medium difference list: > 0.01 times'
	for ll in lst:
		for l0 in ll:
			delta = abs(pdr1[ property_id][ ll[0]] - pdr1[ property_id][l0])
			if delta > 0.01 * abs(pdr1[property_id][ ll[0]]):
				print pdr1[smiles_id][ ll[0]], '-->', 
				print l0, ":", pdr1[ property_id][l0],
				print "Difference ({})".format( delta)				

	print '\n========================================='
	print 'large difference list: > 0.1 times'
	for ll in lst:
		for l0 in ll:
			delta = abs(pdr1[ property_id][ ll[0]] - pdr1[ property_id][l0])
			if delta > 0.1 * abs(pdr1[property_id][ ll[0]]):
				print pdr1[smiles_id][ ll[0]], '-->', 
				print l0, ":", pdr1[ property_id][l0],
				print "Difference ({})".format( delta)				



"""
Class modules are described below,
while function modules are described above.
"""
class _PD_mlr_r0():
	def __init__(self, pdr, y_id = 'Solubility log(mol/L)', smiles_id = 'SMILES', preprocessing = False):
		
		if preprocessing:
			self.xM = jchem.calc_corr( pdr[ smiles_id].tolist())
		else: 
			self.xM = pd_get_fpM( pdr, smiles_id = smiles_id)

		self.yV = pd_get_yV( pdr, y_id = y_id) 

	def set_SVD(self):
		U,d,VT = np.linalg.svd( self.xM)
		self.xM = self.xM * VT.T

	def _val_vseq_mode_rand_r0( self, mode = {'type': 'ridge', 'alpha': 0.5}, rate = 2, disp = True, graph = True):
		"""
		The regression performed directly from the pdr.
		We define mode dictionary to enter various types of optimization method.

		"""
		ly = len( self.yV)
		vseq = jutil.choose( ly, int(ly / rate));

		if mode['type'] == 'ridge':
			r_sqr, RMSE = jutil.mlr_val_vseq_ridge( self.xM, self.yV, vseq, alpha = mode['alpha'], disp = disp, graph = graph)

		return r_sqr, RMSE

	def val_vseq_mode_rand( self, mode = {'type': 'ridge', 'alpha': 0.5}, rate = 2, disp = True, graph = True):
		"""
		The regression performed directly from the pdr.
		We define mode dictionary to enter various types of optimization method.

		"""
		ly = len( self.yV)
		vseq = jutil.choose( ly, int(ly / rate));

		r_sqr, RMSE = self.val_vseq_mode( self.xM, self.yV, vseq, mode = mode, disp = disp, graph = graph)

		return r_sqr, RMSE

	def _val_vseq_mode_r0( self, RM, yE, v_seq, mode = {'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""
		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		#Regression or prediction can be performed by the predefined type such as Ridge.
		if mode['type'] == 'ridge':
			print 'Ridge: alpha =', mode['alpha']
			clf = linear_model.Ridge( alpha = mode['alpha'])
			clf.fit( RMt, yEt)

		if disp: print 'Training result'
		jutil.mlr_show( clf, RMt, yEt, disp = disp, graph = graph)

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.mlr_show( clf, RMv, yEv, disp = disp, graph = graph)

		#if r_sqr < 0:
		#	print 'v_seq:', v_seq, '--> r_sqr = ', r_sqr

		return r_sqr, RMSE


	def _val_vseq_mode_gpnorm( self, RM, yE, v_seq, mode = {'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""
		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		#Regression or prediction can be performed by the predefined type such as Ridge.
		if mode['type'] == 'ridge':
			print 'Ridge: alpha =', mode['alpha']
			clf = linear_model.Ridge( alpha = mode['alpha'])

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

		elif mode['type'] == 'maml_gp':
			RMt_a, yEt_a = np.array( RMt), np.array( yEt) / mode['norm']
			RMv_a, yEv_a = np.array( RMv), np.array( yEv) / mode['norm']
			jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

			# Training mode
			jgp_en.optimize_noise_and_amp()
			yEt_predict = np.mat( jgp_en.predicted_targets)
			print yEt_predict.shape

			# Validation mode
			jgp_en.run_gp()
			yEv_predict = np.mat( jgp_en.predicted_targets)
			print yEv_predict.shape

		#if disp: print 'Training result'
		#jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv / mode['norm'], yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE

	def _val_vseq_mode_r0( self, RM, yE, v_seq, mode = {'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""
		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		#Regression or prediction can be performed by the predefined type such as Ridge.
		if mode['type'] == 'ridge':
			print 'Ridge: alpha =', mode['alpha']
			clf = linear_model.Ridge( alpha = mode['alpha'])

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

		elif mode['type'] == 'maml_gp':
			RMt_a, yEt_a = np.array( RMt), np.array( yEt)
			RMv_a, yEv_a = np.array( RMv), np.array( yEv)
			jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

			# Training mode
			jgp_en.optimize_noise_and_amp()
			yEt_predict = np.mat( jgp_en.predicted_targets)
			print yEt_predict.shape

			# Validation mode
			jgp_en.run_gp()
			yEv_predict = np.mat( jgp_en.predicted_targets)
			print yEv_predict.shape

		if mode['type'] != 'maml_gp':
			if disp: print 'Training result'
			jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv, yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE

	def val_vseq_mode( self, RM, yE, v_seq, mode = {'tool': 'sklearn', 'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""

		if 'tool' not in mode.keys():
			if mode['type'] in ('maml_gp'):
				mode['tool'] = 'AAG'
			else:
				mode['tool'] = 'sklearn'

		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		if mode['tool'] == 'sklearn': 
			if mode['type'] == 'ridge':
				print 'Ridge: alpha =', mode['alpha']
				clf = linear_model.Ridge( alpha = mode['alpha'])
			elif mode['type'] == 'Lasso':
				print 'Lasso: alpha =', mode['alpha']
				clf = linear_model.Lasso( alpha = mode['alpha'])
			elif mode['type'] == 'ElasticNet':
				print 'ElasticNet: alpha = {0}, l1_ratio = {1}'.format( mode['alpha'], mode['l1_ratio'])
				clf = linear_model.ElasticNet( alpha = mode['alpha'], l1_ratio = mode['l1_ratio'], normalize = True)
			elif mode['type'] == 'LassoLars':
				print 'LassoLars: alpha =', mode['alpha']
				clf = linear_model.LassoLars( alpha = mode['alpha'])
			else:
				raise TypeError("The given mode is not supported yet or spells are different.")

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

			if disp: print 'Training result'
			#print yEt_predict[:10] #For debugging			
			jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		elif mode['tool'] == 'AAG':
			if mode['type'] == 'maml_gp':
				RMt_a, yEt_a = np.array( RMt), np.array( yEt)
				RMv_a, yEv_a = np.array( RMv), np.array( yEv)
				jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

				# Training mode
				jgp_en.optimize_noise_and_amp()
				yEt_predict = np.mat( jgp_en.predicted_targets)
				print yEt_predict.shape

				# Validation mode
				jgp_en.run_gp()
				yEv_predict = np.mat( jgp_en.predicted_targets)
				print yEv_predict.shape
		else:
			raise TypeError("{} is not support for mode-tool yet.".format( mode['tool']))

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv, yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE

	def _val_vseq_mode_r1( self, RM, yE, v_seq, mode = {'tool': 'sklearn', 'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""

		if 'tool' not in mode.keys():
			if mode['type'] in ('maml_gp'):
				mode['tool'] = 'AAG'
			else:
				mode['tool'] = 'sklearn'

		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		if mode['tool'] == 'sklearn': 
			if mode['type'] == 'ridge':
				print 'Ridge: alpha =', mode['alpha']
				clf = linear_model.Ridge( alpha = mode['alpha'])
			elif mode['type'] == 'Lasso':
				print 'Lasso: alpha =', mode['alpha']
				clf = linear_model.Lasso( alpha = mode['alpha'])
			elif mode['type'] == 'ElasticNet':
				print 'ElasticNet: alpha = {0}, l1_ratio = {1}'.format( mode['alpha'], mode['l1_ratio'])
				clf = linear_model.ElasticNet( alpha = mode['alpha'], l1_ratio = mode['l1_ratio'])
			elif mode['type'] == 'LassoLars':
				print 'LassoLars: alpha =', mode['alpha']
				clf = linear_model.LassoLars( alpha = mode['alpha'])
			else:
				raise TypeError("The given mode is not supported yet or spells are different.")

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

			if disp: print 'Training result'
			#print yEt_predict[:10] #For debugging			
			jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		elif mode['tool'] == 'AAG':
			if mode['type'] == 'maml_gp':
				RMt_a, yEt_a = np.array( RMt), np.array( yEt)
				RMv_a, yEv_a = np.array( RMv), np.array( yEv)
				jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

				# Training mode
				jgp_en.optimize_noise_and_amp()
				yEt_predict = np.mat( jgp_en.predicted_targets)
				print yEt_predict.shape

				# Validation mode
				jgp_en.run_gp()
				yEv_predict = np.mat( jgp_en.predicted_targets)
				print yEv_predict.shape
		else:
			raise TypeError("{} is not support for mode-tool yet.".format( mode['tool']))

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv, yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE


	def val_vseq_ridge_rand_profile( self, alpha = .5, rate = 2, iterN = 10, disp = False, graph = False, hist = True):
		jutil.mlr_val_vseq_ridge_rand_profile( self.xM, self.yV, alpha = alpha, rate = rate, iterN = iterN, 
			disp = disp, graph = graph, hist = hist)

	def val_vseq_mode_rand_profile( self, mode, rate = 2, iterN = 10, disp = True, graph = False, hist = True):

		RM = self.xM
		yE = self.yV

		r2_rms_list = []
		for ii in range( iterN):
			vseq = jutil.choose( len( yE), int(len( yE) / rate));
			r_sqr, RMSE = self.val_vseq_mode( RM, yE, vseq, mode = mode, disp = disp, graph = graph)
			r2_rms_list.append( (r_sqr, RMSE))

		r2_list, rms_list = zip( *r2_rms_list)

		#Showing r2 as histogram
		pd_r2 = pd.DataFrame( {'r_sqr': r2_list})
		pd_r2.plot( kind = 'hist', alpha = 0.5)

		#Showing rms as histogram
		pd_rms = pd.DataFrame( {'rms': rms_list})
		pd_rms.plot( kind = 'hist', alpha = 0.5)

		print "average r2 and sd:", map( np.mean, [r2_list, rms_list])

		return r2_list, rms_list

class _PD_mlr_r1(): # 2015-6-3
	def __init__(self, pdr, y_id = 'Solubility log(mol/L)', smiles_id = 'SMILES', 
			preprocessing = False, forwardpreprocessing = True):
		"""
		y normalization is not important for prediction. 
		X normalization seems to be useful but not confirmed yet.
		"""

		if preprocessing:
			self.A = jchem.calc_corr( pdr[ smiles_id].tolist())
			self.xM = self.A
		else: 
			self.xM_org = pd_get_fpM( pdr, smiles_id = smiles_id)
			self.xM = self.xM_org

		self.preprocessing = preprocessing
		self.forwardpreprocessing = forwardpreprocessing

		self.yV = pd_get_yV( pdr, y_id = y_id)
		#self.mean_yV = np.mean( yV)
		#self.yV = yV - self.mean_yV

	def set_SVD(self):
		U,d,VT = np.linalg.svd( self.xM)
		self.xM = self.xM * VT.T

	def reset_SVD(self):
		self.xM = self.xM_org

	def val_vseq_mode_seq( self, mode = {'type': 'ridge', 'alpha': 0.5}, st_val = 0, rate = 2, disp = True, graph = True):
		"""
		The regression performed directly from the pdr.
		We define mode dictionary to enter various types of optimization method.

		"""
		ly = len( self.yV)
		vseq = range( st_val, ly, rate)

		r_sqr, RMSE = self.val_vseq_mode( self.xM, self.yV, vseq, mode = mode, disp = disp, graph = graph)

		return r_sqr, RMSE


	def _val_vseq_mode_rand_r0( self, mode = {'type': 'ridge', 'alpha': 0.5}, rate = 2, disp = True, graph = True):
		"""
		The regression performed directly from the pdr.
		We define mode dictionary to enter various types of optimization method.

		"""
		ly = len( self.yV)
		vseq = jutil.choose( ly, int(ly / rate));

		if mode['type'] == 'ridge':
			r_sqr, RMSE = jutil.mlr_val_vseq_ridge( self.xM, self.yV, vseq, alpha = mode['alpha'], disp = disp, graph = graph)

		return r_sqr, RMSE

	def val_vseq_mode_rand( self, mode = {'type': 'ridge', 'alpha': 0.5}, rate = 2, disp = True, graph = True):
		"""
		The regression performed directly from the pdr.
		We define mode dictionary to enter various types of optimization method.

		"""
		ly = len( self.yV)
		vseq = jutil.choose( ly, int(ly / rate));

		r_sqr, RMSE = self.val_vseq_mode( self.xM, self.yV, vseq, mode = mode, disp = disp, graph = graph)

		return r_sqr, RMSE


	def _val_vseq_mode_r0( self, RM, yE, v_seq, mode = {'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""
		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		#Regression or prediction can be performed by the predefined type such as Ridge.
		if mode['type'] == 'ridge':
			print 'Ridge: alpha =', mode['alpha']
			clf = linear_model.Ridge( alpha = mode['alpha'])
			clf.fit( RMt, yEt)

		if disp: print 'Training result'
		jutil.mlr_show( clf, RMt, yEt, disp = disp, graph = graph)

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.mlr_show( clf, RMv, yEv, disp = disp, graph = graph)

		#if r_sqr < 0:
		#	print 'v_seq:', v_seq, '--> r_sqr = ', r_sqr

		return r_sqr, RMSE


	def _val_vseq_mode_gpnorm( self, RM, yE, v_seq, mode = {'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""
		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		#Regression or prediction can be performed by the predefined type such as Ridge.
		if mode['type'] == 'ridge':
			print 'Ridge: alpha =', mode['alpha']
			clf = linear_model.Ridge( alpha = mode['alpha'])

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

		elif mode['type'] == 'maml_gp':
			RMt_a, yEt_a = np.array( RMt), np.array( yEt) / mode['norm']
			RMv_a, yEv_a = np.array( RMv), np.array( yEv) / mode['norm']
			jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

			# Training mode
			jgp_en.optimize_noise_and_amp()
			yEt_predict = np.mat( jgp_en.predicted_targets)
			print yEt_predict.shape

			# Validation mode
			jgp_en.run_gp()
			yEv_predict = np.mat( jgp_en.predicted_targets)
			print yEv_predict.shape

		#if disp: print 'Training result'
		#jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv / mode['norm'], yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE

	def _val_vseq_mode_r0( self, RM, yE, v_seq, mode = {'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""
		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		#Regression or prediction can be performed by the predefined type such as Ridge.
		if mode['type'] == 'ridge':
			print 'Ridge: alpha =', mode['alpha']
			clf = linear_model.Ridge( alpha = mode['alpha'])

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

		elif mode['type'] == 'maml_gp':
			RMt_a, yEt_a = np.array( RMt), np.array( yEt)
			RMv_a, yEv_a = np.array( RMv), np.array( yEv)
			jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

			# Training mode
			jgp_en.optimize_noise_and_amp()
			yEt_predict = np.mat( jgp_en.predicted_targets)
			print yEt_predict.shape

			# Validation mode
			jgp_en.run_gp()
			yEv_predict = np.mat( jgp_en.predicted_targets)
			print yEv_predict.shape

		if mode['type'] != 'maml_gp':
			if disp: print 'Training result'
			jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv, yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE

	def val_vseq_mode( self, RM, yE, v_seq, mode = {'tool': 'sklearn', 'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""

		if 'tool' not in mode.keys():
			if mode['type'] in ('maml_gp'):
				mode['tool'] = 'AAG'
			else:
				mode['tool'] = 'sklearn'

		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		if self.preprocessing and not self.forwardpreprocessing:
			RMt, yEt = RM[ t_seq, :-len(v_seq)], yE[ t_seq, 0]
			RMv, yEv = RM[ v_seq, :-len(v_seq)], yE[ v_seq, 0]			
		else:
			#This is general case
			RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
			RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		if mode['tool'] == 'sklearn': 
			if mode['type'].lower() == 'ridge':
				print 'Ridge: alpha =', mode['alpha']
				clf = linear_model.Ridge( alpha = mode['alpha'])
			elif mode['type'].lower() == 'Lasso'.lower():
				print 'Lasso: alpha =', mode['alpha']
				clf = linear_model.Lasso( alpha = mode['alpha'])
			elif mode['type'].lower() == 'ElasticNet'.lower():
				print 'ElasticNet: alpha = {0}, l1_ratio = {1}'.format( mode['alpha'], mode['l1_ratio'])
				clf = linear_model.ElasticNet( alpha = mode['alpha'], l1_ratio = mode['l1_ratio'], normalize = True)
			elif mode['type'].lower() == 'LassoLars'.lower():
				print 'LassoLars: alpha =', mode['alpha']
				clf = linear_model.LassoLars( alpha = mode['alpha'])
			else:
				raise TypeError("The given mode is not supported yet or spells are different.")

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

			if disp: print 'Training result'
			#print yEt_predict[:10] #For debugging			
			jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		elif mode['tool'] == 'AAG':
			if mode['type'] == 'maml_gp':
				RMt_a, yEt_a = np.array( RMt), np.array( yEt)
				RMv_a, yEv_a = np.array( RMv), np.array( yEv)
				jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

				# Training mode
				jgp_en.optimize_noise_and_amp()
				yEt_predict = np.mat( jgp_en.predicted_targets)
				print yEt_predict.shape

				# Validation mode
				jgp_en.run_gp()
				yEv_predict = np.mat( jgp_en.predicted_targets)
				print yEv_predict.shape
		else:
			raise TypeError("{} is not support for mode-tool yet.".format( mode['tool']))

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv, yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE

	def _val_vseq_mode_r1( self, RM, yE, v_seq, mode = {'tool': 'sklearn', 'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""

		if 'tool' not in mode.keys():
			if mode['type'] in ('maml_gp'):
				mode['tool'] = 'AAG'
			else:
				mode['tool'] = 'sklearn'

		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		if mode['tool'] == 'sklearn': 
			if mode['type'] == 'ridge':
				print 'Ridge: alpha =', mode['alpha']
				clf = linear_model.Ridge( alpha = mode['alpha'])
			elif mode['type'] == 'Lasso':
				print 'Lasso: alpha =', mode['alpha']
				clf = linear_model.Lasso( alpha = mode['alpha'])
			elif mode['type'] == 'ElasticNet':
				print 'ElasticNet: alpha = {0}, l1_ratio = {1}'.format( mode['alpha'], mode['l1_ratio'])
				clf = linear_model.ElasticNet( alpha = mode['alpha'], l1_ratio = mode['l1_ratio'])
			elif mode['type'] == 'LassoLars':
				print 'LassoLars: alpha =', mode['alpha']
				clf = linear_model.LassoLars( alpha = mode['alpha'])
			else:
				raise TypeError("The given mode is not supported yet or spells are different.")

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

			if disp: print 'Training result'
			#print yEt_predict[:10] #For debugging			
			jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		elif mode['tool'] == 'AAG':
			if mode['type'] == 'maml_gp':
				RMt_a, yEt_a = np.array( RMt), np.array( yEt)
				RMv_a, yEv_a = np.array( RMv), np.array( yEv)
				jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

				# Training mode
				jgp_en.optimize_noise_and_amp()
				yEt_predict = np.mat( jgp_en.predicted_targets)
				print yEt_predict.shape

				# Validation mode
				jgp_en.run_gp()
				yEv_predict = np.mat( jgp_en.predicted_targets)
				print yEv_predict.shape
		else:
			raise TypeError("{} is not support for mode-tool yet.".format( mode['tool']))

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv, yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE


	def val_vseq_ridge_rand_profile( self, alpha = .5, rate = 2, iterN = 10, disp = False, graph = False, hist = True):
		jutil.mlr_val_vseq_ridge_rand_profile( self.xM, self.yV, alpha = alpha, rate = rate, iterN = iterN, 
			disp = disp, graph = graph, hist = hist)

	def val_vseq_mode_rand_profile( self, mode, rate = 2, iterN = 10, disp = True, graph = False, hist = True):

		RM = self.xM
		yE = self.yV

		r2_rms_list = []
		for ii in range( iterN):
			vseq = jutil.choose( len( yE), int(len( yE) / rate));
			r_sqr, RMSE = self.val_vseq_mode( RM, yE, vseq, mode = mode, disp = disp, graph = graph)
			r2_rms_list.append( (r_sqr, RMSE))

		r2_list, rms_list = zip( *r2_rms_list)

		#Showing r2 as histogram
		pd_r2 = pd.DataFrame( {'r_sqr': r2_list})
		pd_r2.plot( kind = 'hist', alpha = 0.5)

		#Showing rms as histogram
		pd_rms = pd.DataFrame( {'rms': rms_list})
		pd_rms.plot( kind = 'hist', alpha = 0.5)

		print "average r2 and sd:", map( np.mean, [r2_list, rms_list])

		return r2_list, rms_list

	def predict( self, new_smiles, mode = {'tool': 'sklearn', 'type': 'ridge', 'alpha': 0.5}):
		"""
		predict for new smiles codes
		"""
		if mode['type'].lower() == 'ridge':
			clf = linear_model.Ridge( alpha = mode['alpha'])
		else:
			raise TypeError('The requested mode is not supported yet.')

		#Find an weight vector
		clf.fit( self.xM, self.yV)

		#Predict for new molecules
		new_xM = jchem.gfpM( new_smiles)
		new_yV_pred = clf.predict( new_xM)

		return new_yV_pred

class PD_mlr(): # 2015-6-3
	def __init__(self, pdr, y_id = 'Solubility log(mol/L)', smiles_id = 'SMILES', 
			preprocessing = False, forwardpreprocessing = True):
		"""
		y normalization is not important for prediction. 
		X normalization seems to be useful but not confirmed yet.
		"""

		if preprocessing:
			self.A = jchem.calc_corr( pdr[ smiles_id].tolist())
			self.xM = self.A
		else: 
			self.xM_org = pd_get_fpM( pdr, smiles_id = smiles_id)
			self.xM = self.xM_org

		self.preprocessing = preprocessing
		self.forwardpreprocessing = forwardpreprocessing

		self.yV = pd_get_yV( pdr, y_id = y_id)
		#self.mean_yV = np.mean( yV)
		#self.yV = yV - self.mean_yV

	def set_SVD(self):
		U,d,VT = np.linalg.svd( self.xM)
		self.xM = self.xM * VT.T

	def reset_SVD(self):
		self.xM = self.xM_org

	def val_vseq_mode_seq( self, mode = {'type': 'ridge', 'alpha': 0.5}, st_val = 0, rate = 2, disp = True, graph = True):
		"""
		The regression performed directly from the pdr.
		We define mode dictionary to enter various types of optimization method.

		"""
		ly = len( self.yV)
		vseq = range( st_val, ly, rate)

		r_sqr, RMSE = self.val_vseq_mode( self.xM, self.yV, vseq, mode = mode, disp = disp, graph = graph)

		return r_sqr, RMSE


	def _val_vseq_mode_rand_r0( self, mode = {'type': 'ridge', 'alpha': 0.5}, rate = 2, disp = True, graph = True):
		"""
		The regression performed directly from the pdr.
		We define mode dictionary to enter various types of optimization method.

		"""
		ly = len( self.yV)
		vseq = jutil.choose( ly, int(ly / rate));

		if mode['type'] == 'ridge':
			r_sqr, RMSE = jutil.mlr_val_vseq_ridge( self.xM, self.yV, vseq, alpha = mode['alpha'], disp = disp, graph = graph)

		return r_sqr, RMSE

	def val_vseq_mode_rand( self, mode = {'type': 'ridge', 'alpha': 0.5}, rate = 2, disp = True, graph = True):
		"""
		The regression performed directly from the pdr.
		We define mode dictionary to enter various types of optimization method.

		"""
		ly = len( self.yV)
		vseq = jutil.choose( ly, int(ly / rate));

		r_sqr, RMSE = self.val_vseq_mode( self.xM, self.yV, vseq, mode = mode, disp = disp, graph = graph)

		return r_sqr, RMSE


	def _val_vseq_mode_r0( self, RM, yE, v_seq, mode = {'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""
		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		#Regression or prediction can be performed by the predefined type such as Ridge.
		if mode['type'] == 'ridge':
			print 'Ridge: alpha =', mode['alpha']
			clf = linear_model.Ridge( alpha = mode['alpha'])
			clf.fit( RMt, yEt)

		if disp: print 'Training result'
		jutil.mlr_show( clf, RMt, yEt, disp = disp, graph = graph)

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.mlr_show( clf, RMv, yEv, disp = disp, graph = graph)

		#if r_sqr < 0:
		#	print 'v_seq:', v_seq, '--> r_sqr = ', r_sqr

		return r_sqr, RMSE


	def _val_vseq_mode_gpnorm( self, RM, yE, v_seq, mode = {'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""
		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		#Regression or prediction can be performed by the predefined type such as Ridge.
		if mode['type'] == 'ridge':
			print 'Ridge: alpha =', mode['alpha']
			clf = linear_model.Ridge( alpha = mode['alpha'])

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

		elif mode['type'] == 'maml_gp':
			RMt_a, yEt_a = np.array( RMt), np.array( yEt) / mode['norm']
			RMv_a, yEv_a = np.array( RMv), np.array( yEv) / mode['norm']
			jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

			# Training mode
			jgp_en.optimize_noise_and_amp()
			yEt_predict = np.mat( jgp_en.predicted_targets)
			print yEt_predict.shape

			# Validation mode
			jgp_en.run_gp()
			yEv_predict = np.mat( jgp_en.predicted_targets)
			print yEv_predict.shape

		#if disp: print 'Training result'
		#jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv / mode['norm'], yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE

	def _val_vseq_mode_r0( self, RM, yE, v_seq, mode = {'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""
		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		#Regression or prediction can be performed by the predefined type such as Ridge.
		if mode['type'] == 'ridge':
			print 'Ridge: alpha =', mode['alpha']
			clf = linear_model.Ridge( alpha = mode['alpha'])

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

		elif mode['type'] == 'maml_gp':
			RMt_a, yEt_a = np.array( RMt), np.array( yEt)
			RMv_a, yEv_a = np.array( RMv), np.array( yEv)
			jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

			# Training mode
			jgp_en.optimize_noise_and_amp()
			yEt_predict = np.mat( jgp_en.predicted_targets)
			print yEt_predict.shape

			# Validation mode
			jgp_en.run_gp()
			yEv_predict = np.mat( jgp_en.predicted_targets)
			print yEv_predict.shape

		if mode['type'] != 'maml_gp':
			if disp: print 'Training result'
			jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv, yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE

	def val_vseq_mode( self, RM, yE, v_seq, mode = {'tool': 'sklearn', 'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""

		if 'tool' not in mode.keys():
			if mode['type'] in ('maml_gp'):
				mode['tool'] = 'AAG'
			else:
				mode['tool'] = 'sklearn'

		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		if self.preprocessing and not self.forwardpreprocessing:
			RMt, yEt = RM[ t_seq, :-len(v_seq)], yE[ t_seq, 0]
			RMv, yEv = RM[ v_seq, :-len(v_seq)], yE[ v_seq, 0]			
		else:
			#This is general case
			RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
			RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		if mode['tool'] == 'sklearn': 
			if mode['type'].lower() == 'ridge':
				print 'Ridge: alpha =', mode['alpha']
				clf = linear_model.Ridge( alpha = mode['alpha'])
			elif mode['type'].lower() == 'Lasso'.lower():
				print 'Lasso: alpha =', mode['alpha']
				clf = linear_model.Lasso( alpha = mode['alpha'])
			elif mode['type'].lower() == 'ElasticNet'.lower():
				print 'ElasticNet: alpha = {0}, l1_ratio = {1}'.format( mode['alpha'], mode['l1_ratio'])
				clf = linear_model.ElasticNet( alpha = mode['alpha'], l1_ratio = mode['l1_ratio'], normalize = True)
			elif mode['type'].lower() == 'LassoLars'.lower():
				print 'LassoLars: alpha =', mode['alpha']
				clf = linear_model.LassoLars( alpha = mode['alpha'])
			else:
				raise TypeError("The given mode is not supported yet or spells are different.")

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

			if disp: print 'Training result'
			#print yEt_predict[:10] #For debugging			
			jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		elif mode['tool'] == 'AAG':
			if mode['type'] == 'maml_gp':
				RMt_a, yEt_a = np.array( RMt), np.array( yEt)
				RMv_a, yEv_a = np.array( RMv), np.array( yEv)
				jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

				# Training mode
				jgp_en.optimize_noise_and_amp()
				yEt_predict = np.mat( jgp_en.predicted_targets)
				print yEt_predict.shape

				# Validation mode
				jgp_en.run_gp()
				yEv_predict = np.mat( jgp_en.predicted_targets)
				print yEv_predict.shape
		else:
			raise TypeError("{} is not support for mode-tool yet.".format( mode['tool']))

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv, yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE

	def _val_vseq_mode_r1( self, RM, yE, v_seq, mode = {'tool': 'sklearn', 'type': 'ridge', 'alpha': 0.5}, disp = True, graph = True):
		"""
		Validation is peformed using vseq indexed values.
		"""

		if 'tool' not in mode.keys():
			if mode['type'] in ('maml_gp'):
				mode['tool'] = 'AAG'
			else:
				mode['tool'] = 'sklearn'

		org_seq = range( len( yE))
		t_seq = filter( lambda x: x not in v_seq, org_seq)

		RMt, yEt = RM[ t_seq, :], yE[ t_seq, 0]
		RMv, yEv = RM[ v_seq, :], yE[ v_seq, 0]

		if mode['tool'] == 'sklearn': 
			if mode['type'] == 'ridge':
				print 'Ridge: alpha =', mode['alpha']
				clf = linear_model.Ridge( alpha = mode['alpha'])
			elif mode['type'] == 'Lasso':
				print 'Lasso: alpha =', mode['alpha']
				clf = linear_model.Lasso( alpha = mode['alpha'])
			elif mode['type'] == 'ElasticNet':
				print 'ElasticNet: alpha = {0}, l1_ratio = {1}'.format( mode['alpha'], mode['l1_ratio'])
				clf = linear_model.ElasticNet( alpha = mode['alpha'], l1_ratio = mode['l1_ratio'])
			elif mode['type'] == 'LassoLars':
				print 'LassoLars: alpha =', mode['alpha']
				clf = linear_model.LassoLars( alpha = mode['alpha'])
			else:
				raise TypeError("The given mode is not supported yet or spells are different.")

			# Training mode
			clf.fit( RMt, yEt)
			yEt_predict = clf.predict( RMt)

			# Validation mode
			yEv_predict = clf.predict( RMv)

			if disp: print 'Training result'
			#print yEt_predict[:10] #For debugging			
			jutil.regress_show( yEt, yEt_predict, disp = disp, graph = graph)

		elif mode['tool'] == 'AAG':
			if mode['type'] == 'maml_gp':
				RMt_a, yEt_a = np.array( RMt), np.array( yEt)
				RMv_a, yEv_a = np.array( RMv), np.array( yEv)
				jgp_en = gp.GaussianProcess( RMt_a, yEt_a, RMv_a, yEv_a)

				# Training mode
				jgp_en.optimize_noise_and_amp()
				yEt_predict = np.mat( jgp_en.predicted_targets)
				print yEt_predict.shape

				# Validation mode
				jgp_en.run_gp()
				yEv_predict = np.mat( jgp_en.predicted_targets)
				print yEv_predict.shape
		else:
			raise TypeError("{} is not support for mode-tool yet.".format( mode['tool']))

		if disp: print 'Validation result'
		r_sqr, RMSE = jutil.regress_show( yEv, yEv_predict, disp = disp, graph = graph)

		return r_sqr, RMSE


	def val_vseq_ridge_rand_profile( self, alpha = .5, rate = 2, iterN = 10, disp = False, graph = False, hist = True):
		jutil.mlr_val_vseq_ridge_rand_profile( self.xM, self.yV, alpha = alpha, rate = rate, iterN = iterN, 
			disp = disp, graph = graph, hist = hist)

	def val_vseq_mode_rand_profile( self, mode, rate = 2, iterN = 10, disp = True, graph = False, hist = True):

		RM = self.xM
		yE = self.yV

		r2_rms_list = []
		for ii in range( iterN):
			vseq = jutil.choose( len( yE), int(len( yE) / rate));
			r_sqr, RMSE = self.val_vseq_mode( RM, yE, vseq, mode = mode, disp = disp, graph = graph)
			r2_rms_list.append( (r_sqr, RMSE))

		r2_list, rms_list = zip( *r2_rms_list)

		#Showing r2 as histogram
		pd_r2 = pd.DataFrame( {'r_sqr': r2_list})
		pd_r2.plot( kind = 'hist', alpha = 0.5)

		#Showing rms as histogram
		pd_rms = pd.DataFrame( {'rms': rms_list})
		pd_rms.plot( kind = 'hist', alpha = 0.5)

		print "average r2 and sd:", map( np.mean, [r2_list, rms_list])

		return r2_list, rms_list

	def predict( self, new_smiles, mode = {'tool': 'sklearn', 'type': 'ridge', 'alpha': 0.5}):
		"""
		predict for new smiles codes
		"""
		if mode['type'].lower() == 'ridge':
			clf = linear_model.Ridge( alpha = mode['alpha'])
		else:
			raise TypeError('The requested mode is not supported yet.')

		#Find an weight vector
		clf.fit( self.xM, self.yV)

		#Predict for new molecules
		new_xM = jchem.gfpM( new_smiles)
		new_yV_pred = clf.predict( new_xM)

		return new_yV_pred

class PD_Regress(PD_mlr): # 2015-6-3
	"""
	Extended from PR_mlr for including high-level prediction functionality. 
	"""
	def __init__(self, pdr, y_id = 'Solubility log(mol/L)', smiles_id = 'SMILES', 
			preprocessing = False, forwardpreprocessing = True):
		"""
		y normalization is not important for prediction. 
		X normalization seems to be useful but not confirmed yet.
		  X normalization can be done per column basis for feature regression
		  and done per row basis for molecule basis 
		"""

		self.pdr = pdr
		self.smiles_id = smiles_id

		self.preprocessing = preprocessing
		self.forwardpreprocessing = forwardpreprocessing

		self.xM_org = pd_get_fpM( pdr, smiles_id = smiles_id)
		if preprocessing:
			self.A = jchem.calc_corr( pdr[ smiles_id].tolist())
			self.xM = self.A
		else: 
			self.xM = self.xM_org

		self.preprocessing = preprocessing
		self.forwardpreprocessing = forwardpreprocessing

		self.yV = pd_get_yV( pdr, y_id = y_id)

	def predict( self, new_smiles, mode = {'tool': 'sklearn', 'type': 'ridge', 'alpha': 0.5}, addDescriptor = True):
		# print "Predict in PD_Regress is performing now."

		if mode['type'].lower() == 'ridge':
			clf = linear_model.Ridge( alpha = mode['alpha'])
		else:
			raise TypeError('The requested mode is not supported yet.')

		if self.preprocessing:
			ss_merge = self.pdr[ self.smiles_id].tolist() + new_smiles
			A_merge_all = jchem.calc_corr( ss_merge)
			
			if self.forwardpreprocessing:
				A_merge = A_merge_all[:self.yV.shape[0],:]
				A_new = A_merge_all[self.yV.shape[0]:,:]
			else:
				A_merge = A_merge_all[:self.yV.shape[0],:self.yV.shape[0]]
				A_new = A_merge_all[self.yV.shape[0]:,:self.yV.shape[0]]

			if addDescriptor:
				molw_l = [ Chem.rdMolDescriptors.CalcExactMolWt( Chem.MolFromSmiles(x)) for x in ss_merge]
				Features_merge = jchem.add_new_descriptor( A_merge, molw_l[:self.yV.shape[0]])
				Features_new = jchem.add_new_descriptor( A_new, molw_l[self.yV.shape[0]:])
			else:
				# Original features set are used
				Features_merge = A_merge
				Features_new = A_new

			#clf.fit( A_merge, self.yV)
			#new_yV_pred = clf.predict( A_new)
		else:
			#Find an weight vector
			xM_new = jchem.gfpM( new_smiles)

			if addDescriptor:
				ss_merge = self.pdr[ self.smiles_id].tolist() + new_smiles
				molw_l = [ Chem.rdMolDescriptors.CalcExactMolWt( Chem.MolFromSmiles(x)) for x in ss_merge]
				Features_merge = jchem.add_new_descriptor( self.xM, molw_l[:self.yV.shape[0]])
				Features_new = jchem.add_new_descriptor( xM_new, molw_l[self.yV.shape[0]:])
			else:
				Features_merge = self.xM
				Features_new = xM_new

		clf.fit( Features_merge, self.yV)
		new_yV_pred = clf.predict( Features_new)

		return new_yV_pred

def collect_same_sm( sm_l, te_l):
	"""
	It collect property values which have the same SMILES code in the list
	sm_l: list of smiles codes
	te_l: list of property values associated with a smiles code	
	"""
	unique_sm_l = []
	unique_te_l = []
	min_te_l = []
	te_flag = False
	sm_prev = None
	ln = len( sm_l)
	for idx, (sm, te) in enumerate(zip( sm_l, te_l)):
	    if idx == 0:
	        prev_sm = sm
	        prev_te_all = [te]
	    elif idx == ln - 1:
	        if sm != prev_sm: 
	            unique_sm_l.append( prev_sm)
	            unique_te_l.append( prev_te_all)
	            min_te_l.append( min( prev_te_all))

	            unique_sm_l.append( sm)
	            unique_te_l.append( [te])
	            min_te_l.append( te)
	        else:
	            prev_te_all.append( te)

	            unique_sm_l.append( prev_sm)
	            unique_te_l.append( prev_te_all)            
	            min_te_l.append( min( prev_te_all))
	    else: 
	        if sm != prev_sm: 
	            unique_sm_l.append( prev_sm)
	            unique_te_l.append( prev_te_all)
	            min_te_l.append( min( prev_te_all))
	            
	            prev_sm = sm
	            prev_te_all = [te]
	        else:
	            prev_te_all.append( te)
	
	return unique_sm_l, unique_te_l, min_te_l

def pd_collect_same_sm( pdr, y_id = 'total_energy', smiles_id = 'SMILES'):
	sm_l = pdr.SMILES.tolist()
	te_l = pdr.total_energy.tolist()

	unique_sm_l, unique_te_l, min_te_l = collect_same_sm( sm_l, te_l)

	pdw = pd.DataFrame()
	pdw['ID'] = range( 1, len(unique_sm_l)+1)
	pdw['SMILES'] = unique_sm_l
	pdw['min_total_energy'] = min_te_l
	pdw['total_energy'] = unique_te_l

	return pdw

def pdw_collect_same_sm( pdr, fname, y_id = 'total_energy', smiles_id = 'SMILES'):
	pdw = pd_collect_same_sm( pdr, y_id = y_id, smiles_id = smiles_id)
	print 'Size of DataFrame:', pdw.shape

	pdw.to_csv( fname, index = False)